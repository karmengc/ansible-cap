---

- name: Download Scala package
  get_url:
    url: "{{ scala_mirror }}/scala-{{ scala_version }}.rpm"
    dest: "/tmp/scala-{{ scala_version }}.rpm"

- name: Install Java and Scala packages
  yum:
    name:
      - java
      - java-devel
      - "/tmp/scala-{{ scala_version }}.rpm"
    state: present

- include: set_master_conf.yml
  when: ansible_hostname == "master"

- include: set_worker_conf.yml
  when: ansible_hostname == "worker"

- include: authorize_keys.yml

- name: Set etc_hosts
  blockinfile:
    path: "/etc/hosts"
    insertafter: EOF
    block: |
      {{ spark_master_ip }} master
      {{ spark_worker_ip }} worker

- name: Ensure Spark install dir exists
  file:
    path: "{{ spark_install_dir }}"
    mode: 0755
    state: directory
    follow: true
    owner: "{{ spark_user }}"
    group: "{{ spark_user }}"

- name: Download Spark
  get_url:
    url: "{{ spark_mirror }}/spark-{{ spark_version }}.tgz"
    dest: "/tmp/spark-{{ spark_version }}.tgz"

- name: Extract Spark
  unarchive:
    src: "/tmp/spark-{{ spark_version }}.tgz"
    dest: "{{ spark_install_dir }}"
    copy: no
    owner: "{{ spark_user }}"
    group: "{{ spark_user }}"
    creates: "{{ spark_install_dir }}/spark-{{ spark_version }}"

- name: Create symbolic link
  file:
    src: "{{ spark_install_dir }}/spark-{{ spark_version }}"
    path: "/usr/local/spark"
    state: link
    owner: "{{ spark_user }}"
    group: "{{ spark_user }}"

- name: Create symbolic link
  file:
    src: "{{ spark_install_dir }}/spark-{{ spark_version }}"
    path: "/opt/spark"
    state: link
    owner: "{{ spark_user }}"
    group: "{{ spark_user }}"

- name: Set var log spark perms
  file:
    path: "/var/log/spark"
    state: directory
    owner: "{{ spark_user }}"
    group: "{{ spark_user }}"
    mode: 0755

- name: Configure Spark environment
  template:
    src: spark-env.sh.j2
    dest: "{{ spark_install_dir }}/spark-{{ spark_version }}/conf/spark-env.sh"
    owner: "{{ spark_user }}"
  notify: "Restart all spark from master"

#- name: Configure Spark defaults config file
#  template:
#    src: spark-defaults.conf.j2
#    dest: "{{ spark_usr_parent_dir }}/spark-{{ spark_version }}/conf/spark-defaults.conf"

- name: Configure slaves config file
  template:
    src: slaves.j2
    dest: "{{ spark_install_dir }}/spark-{{ spark_version }}/conf/slaves"
    owner: "{{ spark_user }}"
    group: "{{ spark_user }}"
  notify: "Restart all spark from master"


- name: Check if Spark is already runnning
  shell: pidof java
  ignore_errors: yes
  register: spark_running

- debug: var=spark_running.rc

  #- name: Launch start script at Master
  #  command: "su {{ spark_master_user }} {{ spark_install_dir }}/spark-{{ spark_version }}/sbin/start-all.sh"
  #  when: ansible_hostname == "master" and spark_running.rc == 1

- name: Launch start script at Master
  shell: |
    {{ spark_install_dir }}/spark-{{ spark_version }}/sbin/start-all.sh
    become: yes
  become_user: "{{ spark_master_user }}"
  ignore_errors: true
  no_log: true
  when: ansible_hostname == "master" and spark_running.rc == 1

- include: spark-log4j.yml

- include: test_app.yml
  when: ansible_hostname == "master" and spark_test_app == true


# vim: ff=unix:ai:et:sw=2:ts=2:
