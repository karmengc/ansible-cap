---

#- name: Launch test app at master
#  shell: |
#     {{ spark_install_dir }}/spark-{{ spark_version }}/bin/spark-submit --master spark://{{ spark_master_ip }}:7077 --class org.apache.spark.examples.mllib.KMeansExample {{ spark_install_dir }}/spark-{{ spark_version }}/examples/jars/spark-examples_2.11-2.2.0.jar
#  become: yes
#  become_user: "{{ spark_master_user }}"
#  async: 1000
#  poll: 0
#  #no_log: true
#  ignore_errors: true
#  register: test_app_launched
#
#- name: 'Test App Launched - check on async task'
#  async_status:
#    jid: "{{ test_app_launched.ansible_job_id }}"
#  register: job_result
#  until: test_app_launched.finished
#  retries: 30

- name: Copy WordCount library if test is selected
  copy:
    src: "wordCount_2.0.2.jar"
    dest: "/tmp/WordCount.jar"
    owner: "{{ spark_master_user }}"
    group: "{{ spark_master_user }}"

- name: Copy texttxt example file
  copy:
    src: "texto.txt"
    dest: "/tmp/"
    owner: "{{ hadoop_user }}"
    group: "{{ hadoop_user }}"

- name: Create hdfs folder with Standalone spark
  shell: |
    {{ hadoop_install_dir }}/hadoop-{{ hadoop_version }}/bin/hdfs dfsadmin -safemode leave
    {{ hadoop_install_dir }}/hadoop-{{ hadoop_version }}/bin/hdfs dfs -rm -R /prueba
    {{ hadoop_install_dir }}/hadoop-{{ hadoop_version }}/bin/hdfs dfs -mkdir /prueba
    {{ hadoop_install_dir }}/hadoop-{{ hadoop_version }}/bin/hdfs dfs -put /tmp/texto.txt /prueba/texto.txt
  become: yes
  become_user: "{{ hadoop_user }}"
  ignore_errors: yes

  #Esta parte mejor ejecutarla directamente en la m√°quina para ver la salida
  #- name: execute wordcount jar
  #  shell: |
  #    {{ spark_install_dir }}/spark-{{ spark_version }}/bin/spark-submit --class /tmp/WordCount.jar --master mesos://{{ spark_master_ip }}:7077 hdfs://{{ spark_master_ip }}::8020/prueba/texto.txt hdfs://{{ spark_master_ip }}:8020/prueba/resultadoMesos
  #  become: yes
  #  become_user: "{{ spark_master_user }}"
  #  ignore_errors: yes

- name: test app message_1
  debug:
    msg: "Para probar la instalacion HDFS/spark standalone ejecute:
          Acceder a las interfaces http://{{ spark_master_ip }}:8080 (Spark) y
          http://{{ spark_master_ip }}:50070 (HDFS) para comprobar que todo ha ido bien."

- name: test app message_2
  debug:
    msg: "Loguearse con sparkuser y hacer: 
          {{ spark_install_dir }}/spark-{{ spark_version }}/bin/spark-submit --class wordCount /tmp/WordCount.jar --master spark://{{ spark_master_ip }}:7077  hdfs://{{ spark_master_ip }}:8020/prueba/texto.txt hdfs://{{ spark_master_ip }}:8020/prueba/resultadosMesos"

- name: test app message_3
  debug:
    msg: "Probar los siguientes comandos:
          {{ hadoop_install_dir }}/hadoop-{{ hadoop_version }}/bin/hdfs dfs -ls /prueba
          {{ hadoop_install_dir }}/hadoop-{{ hadoop_version }}/bin/hdfs dfs -get /prueba/texto.txt /home/usuario
          {{ hadoop_install_dir }}/hadoop-{{ hadoop_version }}/bin/hdfs dfs -chmod 777 /prueba"

# vim: ff=unix:ai:et:sw=2:ts=2:
