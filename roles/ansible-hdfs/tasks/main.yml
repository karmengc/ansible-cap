---

#- name: Install Java and Scala packages
#  yum:
#    name:
#      - java
#      - java-devel
#    state: present

#- include: set_master_conf.yml
#  when: ansible_hostname == "master"
#
#- include: set_worker_conf.yml
#  when: ansible_hostname == "worker"
#
#- include: authorize_keys.yml

- name: Set etc_hosts
  blockinfile:
    path: "/etc/hosts"
    insertafter: EOF
    block: |
      {{ spark_master_ip }} master
      {{ spark_worker_ip }} worker

- name: Ensure Spark install dir exists
  file:
    path: "{{ spark_install_dir }}"
    mode: 0755
    state: directory
    follow: true
    owner: "{{ spark_user }}"
    group: "{{ spark_user }}"

- name: Download Hadoop
  get_url:
    url: "{{ hadoop_mirror }}/hadoop-{{ hadoop_version }}/hadoop-{{ hadoop_version }}.tar.gz"
    dest: "/tmp/hadoop-{{ hadoop_version }}.tar.gz"

- name: Extract Hadoop
  unarchive:
    src: "/tmp/hadoop-{{ hadoop_version }}.tar.gz"
    dest: "{{ hadoop_install_dir }}"
    copy: no
    owner: "{{ hadoop_user }}"
    group: "{{ hadoop_user }}"
    creates: "{{ hadoop_install_dir }}/hadoop-{{ hadoop_version }}"

- name: Create symbolic link
  file:
    src: "{{ hadoop_install_dir }}/hadoop-{{ hadoop_version }}"
    path: "/usr/local/hadoop"
    state: link
    owner: "{{ hadoop_user }}"
    group: "{{ hadoop_user }}"

- name: Set var log spark perms
  file:
    path: "/var/log/hadoop"
    state: directory
    owner: "{{ hadoop_user }}"
    group: "{{ hadoop_user }}"
    mode: 0755

- name: Configure Hadoop environment
  template:
    src: hadoop-env.sh.j2
    dest: "{{ hadoop_install_dir }}/hadoop-{{ hadoop_version }}/etc/hadoop/hadoop-env.sh"
    owner: "{{ hadoop_user }}"
#  notify: "Restart all hadoop from master"

    #Editar fichero core-site.xml
    #Editar fichero hdfs-site.xml
    #Editar fichero slaves
    #Editar fichero /etc/hosts - hacerlo aquí porque añade cosas de Ipv6 y es diferente en master que en worker
    #y se elimina la línea de localhost sólo en el master
    #formatear stma ficheros hdfs namenode --format
    #levnatar hadoop start-dfs.sh
    #logs de hadoop en /opt/hadoop/logs
    #Tests comandos de prueba


#- name: Configure Spark defaults config file
#  template:
#    src: spark-defaults.conf.j2
#    dest: "{{ spark_usr_parent_dir }}/spark-{{ spark_version }}/conf/spark-defaults.conf"

- name: Configure slaves config file
  template:
    src: slaves.j2
    dest: "{{ spark_install_dir }}/spark-{{ spark_version }}/conf/slaves"
    owner: "{{ spark_user }}"
    group: "{{ spark_user }}"
  notify: "Restart all spark from master"


- name: Check if Spark is already runnning
  shell: pidof java
  ignore_errors: yes
  register: spark_running

- debug: var=spark_running.rc

  #- name: Launch start script at Master
  #  command: "su {{ spark_master_user }} {{ spark_install_dir }}/spark-{{ spark_version }}/sbin/start-all.sh"
  #  when: ansible_hostname == "master" and spark_running.rc == 1

- name: Launch start script at Master
  shell: |
    {{ spark_install_dir }}/spark-{{ spark_version }}/sbin/start-all.sh
    become: yes
  become_user: "{{ spark_master_user }}"
  #ignore_errors: true
  when: ansible_hostname == "master" and spark_running.rc == 1

- include: spark-log4j.yml

- include: test_app.yml
  when: ansible_hostname == "master" and spark_test_app == true


# vim: ff=unix:ai:et:sw=2:ts=2:
