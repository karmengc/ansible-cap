---
- name: Disable Ipv6
  blockinfile:
    block: |
      net.ipv6.conf.all.disable_ipv6 = 1
      net.ipv6.conf.default.disable_ipv6 = 1
      net.ipv6.conf.lo.disable_ipv6 = 1
      net.ipv6.conf.all.disable_ipv6 = 1
      net.ipv6.conf.default.disable_ipv6 = 1
      net.ipv6.conf.lo.disable_ipv6 = 1
    dest: "/etc/sysctl.conf"
    insertafter: EOF

- name: Ensure Hadoop install dir exists
  file:
    path: "{{ hadoop_install_dir }}"
    mode: 0755
    state: directory
    follow: true
    owner: "{{ hadoop_user }}"
    group: "{{ hadoop_user }}"

- name: Download Hadoop
  get_url:
    url: "{{ hadoop_mirror }}/hadoop-{{ hadoop_version }}/hadoop-{{ hadoop_version }}.tar.gz"
    dest: "/tmp/hadoop-{{ hadoop_version }}.tar.gz"

- name: Extract Hadoop
  unarchive:
    src: "/tmp/hadoop-{{ hadoop_version }}.tar.gz"
    dest: "{{ hadoop_install_dir }}"
    copy: no
    owner: "{{ hadoop_user }}"
    group: "{{ hadoop_user }}"
    creates: "{{ hadoop_install_dir }}/hadoop-{{ hadoop_version }}"

- name: Create symbolic link
  file:
    src: "{{ hadoop_install_dir }}/hadoop-{{ hadoop_version }}"
    path: "/usr/local/hadoop"
    state: link
    owner: "{{ hadoop_user }}"
    group: "{{ hadoop_user }}"

- name: Set var log hadoop perms
  file:
    path: "/var/log/hadoop"
    state: directory
    owner: "{{ hadoop_user }}"
    group: "{{ hadoop_user }}"
    mode: 0755

- name: Configure Hadoop environment
  template:
    src: hadoop-env.sh.j2
    dest: "{{ hadoop_install_dir }}/hadoop-{{ hadoop_version }}/etc/hadoop/hadoop-env.sh"
    owner: "{{ hadoop_user }}"
#  notify: "Restart all hadoop from master"

- name: Set bash profile
  blockinfile:
    block: |
      export JAVA_HOME=$(dirname $(dirname $(readlink $(readlink $(which java)))))
      export PATH=$PATH:$JAVA_HOME/bin
      export CLASSPATH=.:$JAVA_HOME/jre/lib:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar
      #Hadoop
      export HADOOP_HOME=/opt/hadoop
      export PATH=$PATH:/opt/hadoop/bin
      export PATH=$PATH:/opt/hadoop/sbin
      export HADOOP_MAPRED_HOME=/opt/hadoop
      export HADOOP_COMMON_HOME=/opt/hadoop
      export HADOOP_HDFS_HOME=/opt/hadoop
      export YARN_HOME=/opt/hadoop
    dest: "/home/{{ hadoop_user }}/.bash_profile"
    owner: "{{ hadoop_user }}"
    group: "{{ hadoop_user }}"
    insertafter: 'EOF'
    create: yes
    state: present



    #Editar fichero core-site.xml
    #Editar fichero hdfs-site.xml
    #Editar fichero slaves
    #Editar fichero /etc/hosts - hacerlo aquí porque añade cosas de Ipv6 y es diferente en master que en worker
    #y se elimina la línea de localhost sólo en el master
    #formatear stma ficheros hdfs namenode --format
    #levnatar hadoop start-dfs.sh
    #logs de hadoop en /opt/hadoop/logs
    #Tests comandos de prueba


#- name: Configure Spark defaults config file
#  template:
#    src: spark-defaults.conf.j2
#    dest: "{{ spark_usr_parent_dir }}/spark-{{ spark_version }}/conf/spark-defaults.conf"

- name: Configure slaves config file
  template:
    src: slaves.j2
    dest: "{{ spark_install_dir }}/spark-{{ spark_version }}/conf/slaves"
    owner: "{{ spark_user }}"
    group: "{{ spark_user }}"
  notify: "Restart all spark from master"


- name: Check if Spark is already runnning
  shell: pidof java
  ignore_errors: yes
  register: spark_running

- debug: var=spark_running.rc

  #- name: Launch start script at Master
  #  command: "su {{ spark_master_user }} {{ spark_install_dir }}/spark-{{ spark_version }}/sbin/start-all.sh"
  #  when: ansible_hostname == "master" and spark_running.rc == 1

- name: Launch start script at Master
  shell: |
    {{ spark_install_dir }}/spark-{{ spark_version }}/sbin/start-all.sh
    become: yes
  become_user: "{{ spark_master_user }}"
  #ignore_errors: true
  when: ansible_hostname == "master" and spark_running.rc == 1

- include: spark-log4j.yml

- include: test_app.yml
  when: ansible_hostname == "master" and spark_test_app == true


# vim: ff=unix:ai:et:sw=2:ts=2:
